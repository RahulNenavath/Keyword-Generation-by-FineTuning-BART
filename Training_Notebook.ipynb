{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d94e2911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahul/miniconda3/envs/keyword-generation/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Device Name: NVIDIA GeForce RTX 3060\n",
      "Sun Sep 11 18:32:37 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.60.02    Driver Version: 512.15       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:04:00.0  On |                  N/A |\n",
      "|  0%   48C    P8    13W / 170W |    646MiB / 12288MiB |      4%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f'Torch Device Name: {torch.cuda.get_device_name()}')\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2365a822-b3ef-43ad-8d97-f34df4e3dc85",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9eafa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rahul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pynvml import *\n",
    "from string import punctuation\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b29eb2-089f-493f-9ece-1e839ade1011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e143d152-b60a-4eb1-a503-b2954518d970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 773 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e2ac5ad-4636-4b38-99c4-c8c34b4437fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = pathlib.Path().resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1678e945-628a-4e33-a602-519c08e3c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(current_dir, 'data/text_keyword_dataset.gzip')\n",
    "\n",
    "# Save the data-indices of multiple batches as PKL file in Google Drive as you don't lose it later\n",
    "data_indices_path = os.path.join(str(current_dir), 'utils/batch_indices.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18919e27-b6cf-45c0-9f12-d8ac55c27dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset size: 298311\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_parquet(dataset_path)\n",
    "print(f'Total Dataset size: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deed629c-5c3a-4487-800a-0e6a1b4c8608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Autonomous resource provisioning for multi-ser...</td>\n",
       "      <td>multi-service application\\nresource provisioni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Collaborative filtering for orkut communities:...</td>\n",
       "      <td>association rule mining\\ncollaborative filteri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A trust management framework for service-orien...</td>\n",
       "      <td>distributed systems\\nreputation\\nsecurity and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Autonomous resource provisioning for multi-ser...   \n",
       "1  Collaborative filtering for orkut communities:...   \n",
       "2  A trust management framework for service-orien...   \n",
       "\n",
       "                                             keyword  \n",
       "0  multi-service application\\nresource provisioni...  \n",
       "1  association rule mining\\ncollaborative filteri...  \n",
       "2  distributed systems\\nreputation\\nsecurity and ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce08794-4ae8-4198-8cc6-73bd3f36a3c3",
   "metadata": {},
   "source": [
    "# Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04249f0d-899c-4651-adff-4b871678ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(text:str): return len(text.split())\n",
    "def remove_newlines(text:str): return re.sub(r'\\n', ';', text) if isinstance(text, str) else ''\n",
    "def remove_tabs(text: str): return re.sub(r'\\t', ' ', text) if isinstance(text, str) else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79d41523-ced3-438c-970d-40dd55190f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some keywords have newline character and some don't. We rather remove newline character from all the keyword sentences\n",
    "\n",
    "dataset['word_count'] = dataset['text'].apply(get_word_count)\n",
    "dataset['text'] = dataset['text'].apply(remove_newlines)\n",
    "dataset['text'] = dataset['text'].apply(remove_tabs)\n",
    "dataset['keyword'] = dataset['keyword'].apply(remove_newlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cb305fe-d0df-4b0c-8d37-4a2bdf61d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many sentences which are less than 25 words. This is not helpful to generate accuracy keywords as we don't have much context\n",
    "\n",
    "indices_to_remove = list(dataset[dataset['word_count']<=25].index)\n",
    "dataset.drop(indices_to_remove, inplace=True)\n",
    "dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bc68d72-381a-4e79-b462-5f14f8395165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_record(index:int, data:pd.DataFrame=dataset):\n",
    "    record = data.iloc[index]\n",
    "    return {'text': record['text'], 'keywords': record['keyword']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bf0ce45-5c4d-4c3b-951c-49479e78cf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Verona Lastre: consolidation provides opening for a new plate vendor;Fewer companies than ever are manufacturing CTP plates. The market has become; globalized, with just four big firms dominating the picture. To the; Samor Group, however, globalization looked like an opportunity; it; reasoned that many a national and local distributor would welcome a; small, competitive, regional manufacturer. A couple of years ago it; formed a company, Verona Lastre, to exploit that opportunity. Now Vela,; as it's familiarly called, has launched its line of high-quality; thermal plates and is busily lining up dealers in Europe and the; Americas;\",\n",
       " 'keywords': 'Verona Lastre;Vela;CTP plates;computer controlled typesetting;printing industry;publishing;'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_record(1030)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4095a38-824f-40c9-9e14-3a0bf0f5ee3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Autonomous resource provisioning for multi-ser...</td>\n",
       "      <td>multi-service application;resource provisioning;</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Collaborative filtering for orkut communities:...</td>\n",
       "      <td>association rule mining;collaborative filterin...</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A trust management framework for service-orien...</td>\n",
       "      <td>distributed systems;reputation;security and pr...</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Autonomous resource provisioning for multi-ser...   \n",
       "1  Collaborative filtering for orkut communities:...   \n",
       "2  A trust management framework for service-orien...   \n",
       "\n",
       "                                             keyword  word_count  \n",
       "0   multi-service application;resource provisioning;         136  \n",
       "1  association rule mining;collaborative filterin...         230  \n",
       "2  distributed systems;reputation;security and pr...         156  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a541aa-ffbd-495e-9667-7e91b611a0fa",
   "metadata": {},
   "source": [
    "We don't need complete dataset to train the model. Defragment the dataset into multiple batches of 2000 records. This helps in quickly training multiple variations of model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6817752c-e7fa-43a7-a35b-786b46153e2d",
   "metadata": {},
   "source": [
    "# Batching & Transform Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "662a5996-a759-459a-9294-ba5a0cf19ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_indices(data_indices:list, batch_size:int=20_000, path:str=data_indices_path):\n",
    "    random.shuffle(data_indices)\n",
    "    batches = len(data_indices) // batch_size\n",
    "    print(f'Number of batches: {batches}')\n",
    "    x = []\n",
    "    i, batch_len = 0, batch_size\n",
    "    for index, _ in enumerate(range(batches)):\n",
    "        x.append(data_indices[i: i+batch_len])\n",
    "        i += batch_len\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72eeadff-9de8-45ae-bef4-190de9619406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl_data(pkl_obj_path):\n",
    "    with open(pkl_obj_path, 'rb') as f:\n",
    "        indices = pickle.load(f)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93238cc9-880c-4137-b579-4d488e1185d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danger: !Run this only once!\n",
    "# generate_batch_indices(data_indices = list(dataset.index), batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b90d9f21-32c9-4071-ab2b-2a227265553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indices = load_pkl_data(data_indices_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d341bf7-1f21-4cd5-b931-a5f1f045b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_processed(batch_num:int, batch_indices:list=batch_indices, data:pd.DataFrame=dataset):\n",
    "    if batch_num > len(batch_indices):\n",
    "        raise(f'No sufficient batches, Please reduce the batch number')\n",
    "    record_indices = batch_indices[batch_num]\n",
    "    sample_dataset = data.iloc[record_indices]\n",
    "    \n",
    "    train_set, test_set = train_test_split(sample_dataset, test_size=0.2, shuffle=True)\n",
    "    train_set, valid_set = train_test_split(train_set, test_size=0.1, shuffle=True)\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_set)\n",
    "    valid_dataset = Dataset.from_pandas(valid_set)\n",
    "    test_dataset = Dataset.from_pandas(test_set)\n",
    "    \n",
    "    dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'valid': valid_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18628fee-a055-4874-b637-01fab9525d2d",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4187e4c0-7a14-4e47-882e-08b59aea3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"keyword: \"\n",
    "max_input_length = 512\n",
    "max_target_length = 20\n",
    "batch_size = 8\n",
    "\n",
    "# Finetune from total scratch\n",
    "# model_checkpoint = \"sshleifer/distilbart-cnn-12-6\"\n",
    "# Finetune from a check point\n",
    "checkpoint_num = \"1000\"\n",
    "model_checkpoint = os.path.join(current_dir, f'model_saves/checkpoint-{checkpoint_num}')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(\"cuda\")\n",
    "def model_init():\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(\"cuda\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dbfc73-aa06-4ce0-98db-b38889e06737",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "285ef34f-57b8-48e0-8fb2-49e49583c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    sentences = nltk.sent_tokenize(text.strip())\n",
    "    sentences_cleaned = [s for sent in sentences for s in sent.split(\"\\n\")]\n",
    "    sentences_cleaned_no_titles = [sent for sent in sentences_cleaned if len(sent) > 0 and sent[-1] in punctuation]\n",
    "    text_cleaned = \"\\n\".join(sentences_cleaned_no_titles)\n",
    "    return text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "087bba67-e0b9-49ff-9924-a333dd7724fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    texts_cleaned = [clean_text(text) for text in examples[\"text\"]]\n",
    "    inputs = [prefix + text for text in texts_cleaned]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    \n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"keyword\"], max_length=max_target_length, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d18b2a2d-5968-47a4-9d8d-7f33cfa06d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.18ba/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.74ba/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.09ba/s]\n"
     ]
    }
   ],
   "source": [
    "batch_number = 138\n",
    "sample_dataset = get_dataset_processed(batch_num=batch_number)\n",
    "sample_tokenized_datasets = sample_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea070b-9a50-4692-8cca-0da52e019af1",
   "metadata": {},
   "source": [
    "# Training Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "332a5d21-57c7-45b7-9b16-5ebc8b265182",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    os.path.join(current_dir, f'model_saves/'),\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=30,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=5, \n",
    "    gradient_checkpointing=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64454f79-20dd-48ac-8da6-5431e31b79a4",
   "metadata": {},
   "source": [
    "# Score computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d1da2e3-1211-43d7-a131-73e52a8f1649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip()))\n",
    "                      for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) \n",
    "                      for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "\n",
    "    # Extract ROUGE f1 scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length to metrics\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id)\n",
    "                      for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047329fb-3fbb-4cbe-bf18-60b701e910dd",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b01a4562-3e15-47f4-99b8-3de984472ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-1000/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"/home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-1000\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-1000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-1000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=sample_tokenized_datasets[\"train\"],\n",
    "    eval_dataset=sample_tokenized_datasets[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0040e947-be91-4f93-8442-06906da3efa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-1000/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"/home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-1000\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-1000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-1000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: word_count, text, __index_level_0__, keyword. If word_count, text, __index_level_0__, keyword are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/rahul/miniconda3/envs/keyword-generation/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1440\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 40\n",
      "  Gradient Accumulation steps = 5\n",
      "  Total optimization steps = 1080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 55:51, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.978100</td>\n",
       "      <td>1.573347</td>\n",
       "      <td>34.802500</td>\n",
       "      <td>18.695300</td>\n",
       "      <td>28.201300</td>\n",
       "      <td>28.195200</td>\n",
       "      <td>57.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.258600</td>\n",
       "      <td>1.827665</td>\n",
       "      <td>32.962600</td>\n",
       "      <td>17.342500</td>\n",
       "      <td>26.868800</td>\n",
       "      <td>27.085100</td>\n",
       "      <td>57.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.093100</td>\n",
       "      <td>2.171056</td>\n",
       "      <td>33.306300</td>\n",
       "      <td>18.095600</td>\n",
       "      <td>27.377900</td>\n",
       "      <td>27.614000</td>\n",
       "      <td>57.306200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>2.215236</td>\n",
       "      <td>32.373800</td>\n",
       "      <td>17.805900</td>\n",
       "      <td>26.377300</td>\n",
       "      <td>26.756400</td>\n",
       "      <td>57.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>2.332826</td>\n",
       "      <td>32.561000</td>\n",
       "      <td>17.192600</td>\n",
       "      <td>26.716600</td>\n",
       "      <td>26.872000</td>\n",
       "      <td>57.256200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>2.366807</td>\n",
       "      <td>32.214300</td>\n",
       "      <td>17.344000</td>\n",
       "      <td>26.269300</td>\n",
       "      <td>26.495500</td>\n",
       "      <td>57.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>2.507130</td>\n",
       "      <td>32.963700</td>\n",
       "      <td>17.425400</td>\n",
       "      <td>27.467300</td>\n",
       "      <td>27.780100</td>\n",
       "      <td>57.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>2.401115</td>\n",
       "      <td>33.796200</td>\n",
       "      <td>17.880200</td>\n",
       "      <td>27.746700</td>\n",
       "      <td>27.939200</td>\n",
       "      <td>57.106200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>2.429439</td>\n",
       "      <td>33.177900</td>\n",
       "      <td>17.207300</td>\n",
       "      <td>27.544800</td>\n",
       "      <td>27.769400</td>\n",
       "      <td>57.093800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>2.388860</td>\n",
       "      <td>33.028200</td>\n",
       "      <td>17.476400</td>\n",
       "      <td>27.029300</td>\n",
       "      <td>27.238300</td>\n",
       "      <td>57.112500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: word_count, text, __index_level_0__, keyword. If word_count, text, __index_level_0__, keyword are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 160\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: word_count, text, __index_level_0__, keyword. If word_count, text, __index_level_0__, keyword are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 160\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-200\n",
      "Configuration saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-200/config.json\n",
      "Model weights saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-200/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: word_count, text, __index_level_0__, keyword. If word_count, text, __index_level_0__, keyword are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 160\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: word_count, text, __index_level_0__, keyword. If word_count, text, __index_level_0__, keyword are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 160\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-400\n",
      "Configuration saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-400/config.json\n",
      "Model weights saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-400/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: word_count, text, __index_level_0__, keyword. If word_count, text, __index_level_0__, keyword are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 160\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: word_count, text, __index_level_0__, keyword. If word_count, text, __index_level_0__, keyword are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 160\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-600\n",
      "Configuration saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-600/config.json\n",
      "Model weights saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: word_count, text, __index_level_0__, keyword. If word_count, text, __index_level_0__, keyword are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 160\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: word_count, text, __index_level_0__, keyword. If word_count, text, __index_level_0__, keyword are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 160\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-800\n",
      "Configuration saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-800/config.json\n",
      "Model weights saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: word_count, text, __index_level_0__, keyword. If word_count, text, __index_level_0__, keyword are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 160\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: word_count, text, __index_level_0__, keyword. If word_count, text, __index_level_0__, keyword are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 160\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-1000\n",
      "Configuration saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-1000/config.json\n",
      "Model weights saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/rahul/rahul-work/keyword_gen_project/model_saves/checkpoint-800 (score: 33.7962).\n"
     ]
    }
   ],
   "source": [
    "result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6458283-eae5-492a-a747-ca2e040a5222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3349.75\n",
      "Samples/second: 12.90\n",
      "GPU memory occupied: 7934 MB.\n"
     ]
    }
   ],
   "source": [
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "249f143b-ba5a-4896-8ecd-37e0587eb2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir f'{os.path.join(current_dir,\"/model_saves/runs\")}'\n",
    "# !kill 21574"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6bb938-64e3-414a-8fb3-43d159445d75",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92ef31e9-31ab-4bb0-a1e9-3799495b9b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_num = \"800\"\n",
    "model_checkpoint = os.path.join(current_dir, f'model_saves/checkpoint-{checkpoint_num}')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6c94d56-b6dd-48c4-ac56-3d7c962e0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_infernce(text, model, tokenizer) -> str:\n",
    "    \n",
    "    def defrag_sents(text:str, word_len:int=100):\n",
    "        words = nltk.word_tokenize(text)\n",
    "        results = []\n",
    "        c = 0\n",
    "        while c <= len(words):\n",
    "            results.append(\" \".join(words[c:c+word_len]))\n",
    "            c += word_len\n",
    "        return results\n",
    "    \n",
    "    sentences = defrag_sents(text)\n",
    "    keywords = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        text = re.sub(\"\\n\", \" \", sent)\n",
    "        inputs = [\"keyword: \" + text]\n",
    "        inputs = tokenizer(inputs, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "        output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=10, max_length=20)\n",
    "        decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "        predicted_keyword = nltk.sent_tokenize(decoded_output.strip())[0]\n",
    "\n",
    "        keywords.append(predicted_keyword)\n",
    "    return \";\".join(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6d92ce4-ff38-4979-a215-d2cb5c60ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" Former senior Congress leader, Ghulam Nabi Azad said on Sunday that he has not promised restoration of Article 370 in his new political agenda because he doesn`t believe in making false promises. Addressing a public meeting in north Kashmir`s Baramulla town, Azad said, \"To restore Article 370 would need around 350 votes in the Lok Sabha and 175 votes in the Rajya Sabha.\n",
    "\n",
    "\"This is a number no political party has or is likely to ever get. The Congress has shrunken to less than 50 seats and if they speak of restoring Article 370, they are making false promises.\"\n",
    "\n",
    "He said his political agenda includes restoration of statehood, land and jobs for the locals as these are achievable objectives.\n",
    "\n",
    "\"Some people have blamed me for voting in favour of the Article 370 abrogation motion brought in by the home minister.\n",
    "\n",
    "\"I have voted against the abrogation and these people who have no idea about the working of Parliament are saying that I voted against Article 370,\" he said.\n",
    "\n",
    "He said when he was the chief minister of the state, he arrested 13 police personnel for staging a fake encounter in which three persons were killed.\n",
    "\n",
    "\"The arrested persons are in jail for the last 15 years,\" he said.\n",
    "\n",
    "He spoke of the developmental works and creation of districts during his tenure as the chief minister.\n",
    "\n",
    "\"Four new districts were created in the Valley and three in the Jammu division during my tenure as the chief minister. I got new medical colleges during that period.\n",
    "\n",
    "\"Whether I get four votes or lakhs of votes during the elections, I will never deceive the people,\" he assured the public gathering.\n",
    "\n",
    "This was Azad`s first public meeting in Kashmir after he resigned from the basic membership of the Congress party.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04da21da-abd5-4104-9633-d94d930024e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politics and Government: Present: False\n",
      "\n",
      "Azad Ghulam Nabi: Present: False\n",
      "\n",
      " Article 370: Present: False\n",
      "\n",
      ": Present: False\n",
      "\n",
      "Azad (India): Present: False\n",
      "\n",
      "Kashmir and Jammu: Present: False\n",
      "\n",
      "Land Use Act: Present: False\n",
      "\n",
      "Congress (India: Present: False\n",
      "\n",
      "Article 370 (Currency): Present: False\n",
      "\n",
      "India: Present: False\n",
      "\n",
      "Jammu and Jammu: Present: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keywords = model_infernce(text, model, tokenizer)\n",
    "keywords = list(set(keywords.split(';')))\n",
    "\n",
    "input_words = nltk.word_tokenize(text)\n",
    "for item in keywords:\n",
    "    print(f'{item}: Present: {True if item in input_words else False}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90fcca53-60ca-4e7f-8947-6d9b94f4e244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Present: False\n",
      "\n",
      "DC Comics: Present: False\n",
      "\n",
      "Politics and Government: Present: False\n",
      "\n",
      "Decisions and Verdicts: Present: False\n",
      "\n",
      "News media,journalism: Present: False\n",
      "\n",
      "Captain Marvel: Present: False\n",
      "\n",
      "Wawcett Comics: Present: False\n",
      "\n",
      "NYC: Present: False\n",
      "\n",
      " copyright: Present: False\n",
      "\n",
      "Superhero: Present: False\n",
      "\n",
      "Superheroes: Present: False\n",
      "\n",
      "Copyrights: Present: False\n",
      "\n",
      "computers: Present: False\n",
      "\n",
      "New York State: Present: False\n",
      "\n",
      "Computers and the Internet: Present: False\n",
      "\n",
      "Waw: Present: False\n",
      "\n",
      "Newspapers: Present: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keywords = model_infernce(text, model, tokenizer)\n",
    "keywords = list(set(keywords.split(';')))\n",
    "\n",
    "input_words = nltk.word_tokenize(text)\n",
    "for item in keywords:\n",
    "    print(f'{item}: Present: {True if item in input_words else False}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8812e72a-c68d-483a-8045-4b3d18d97747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Present: False\n",
      "\n",
      "Copyrights: Present: False\n",
      "\n",
      "Tech Industry: Present: False\n",
      "\n",
      "Fawcett: Present: True\n",
      "\n",
      "Computers and the Internet: Present: False\n",
      "\n",
      "Superhero: Present: False\n",
      "\n",
      "Superman: Present: True\n",
      "\n",
      "DC Comics: Present: False\n",
      "\n",
      "Captain Marvel: Present: False\n",
      "\n",
      "volution: Present: False\n",
      "\n",
      "pionage: Present: False\n",
      "\n",
      "wedcett Comics: Present: False\n",
      "\n",
      "Copyrights and Copyright Violations: Present: False\n",
      "\n",
      "copyrights: Present: True\n",
      "\n",
      "Fawcett Comics: Present: False\n",
      "\n",
      "Wawcett Comics: Present: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keywords = model_infernce(text, model, tokenizer)\n",
    "keywords = list(set(keywords.split(';')))\n",
    "\n",
    "input_words = nltk.word_tokenize(text)\n",
    "for item in keywords:\n",
    "    print(f'{item}: Present: {True if item in input_words else False}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22240018-4c4d-4c03-928f-224bdd7f67a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 747 MB.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4df61a7b-1f15-4322-98aa-51ea77d3b9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "826e1044-caff-4c50-82f3-f1e702e355c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c15c94-40b5-444b-b82e-914032a25899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keyword-generation] *",
   "language": "python",
   "name": "conda-env-keyword-generation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
