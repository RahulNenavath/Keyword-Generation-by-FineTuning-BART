{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "210202b7-3b89-42ae-b389-36d53da86c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.7.1\n",
      "cuda available: True\n",
      "torch cuda: 12.9\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "from typing import Dict, Optional, List, Any, Tuple, Union\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset, DatasetDict, load_dataset, Features, Value, Sequence\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.project_dir = Path(os.getcwd()).parent\n",
    "        self.data_dir = self.project_dir / 'Data'\n",
    "\n",
    "configobj = Config()\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"torch cuda:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2db62ec0-84cb-4b54-9394-cf88c94c5175",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    # Provide an already-prepared DatasetDict OR instruct how to load/format one.\n",
    "    hf_path_or_none: Optional[str] = None  # e.g. path from save_to_disk OR HF hub id\n",
    "    text_field: str = \"text\"\n",
    "    keywords_field: str = \"keywords\"  # list[str]\n",
    "    max_train_samples: Optional[int] = None\n",
    "    max_eval_samples: Optional[int] = 1000  # to keep eval fast\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PromptConfig:\n",
    "    # Generic prompt template; keep KEYWORDS: as the \"response tag\" for the collator\n",
    "    system_preamble: str = (\n",
    "        \"You are an expert keyword generator. \"\n",
    "        \"Extract concise, relevant keywords for the document below.\"\n",
    "    )\n",
    "    response_tag: str = \"KEYWORDS:\"\n",
    "    sep: str = \"; \"  # how labels are joined for the target text\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    model_id: str = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "    output_dir: str = \"runs/kwgen\"\n",
    "    load_in_4bit: bool = False\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: Optional[List[str]] = None\n",
    "    max_seq_len: int = 2048\n",
    "    per_device_train_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    num_train_epochs: int = 2\n",
    "    learning_rate: float = 2e-4\n",
    "    logging_steps: int = 50\n",
    "    eval_steps: int = 500\n",
    "    save_steps: int = 500\n",
    "    warmup_ratio: float = 0.03\n",
    "    weight_decay: float = 0.0\n",
    "    seed: int = 42\n",
    "    bf16: bool = False\n",
    "    fp16: bool = True\n",
    "    report_to: Optional[str] = None\n",
    "    gradient_checkpointing: bool = True\n",
    "    save_total_limit: int = 2\n",
    "    optim: str = \"adamw_torch\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08db3d43-2196-4e1d-8c5a-3c37c322869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_kw_string(s: str) -> List[str]:\n",
    "    if s is None:\n",
    "        return []\n",
    "    raw = []\n",
    "    for chunk in s.split(\"\\n\"):\n",
    "        raw.extend([p for w in chunk.split(\";\") for p in w.split(\",\")])\n",
    "    norm, seen = [], set()\n",
    "    for k in raw:\n",
    "        k2 = k.strip()\n",
    "        kn = k2.casefold()\n",
    "        if k2 and kn not in seen:\n",
    "            seen.add(kn)\n",
    "            norm.append(k2)\n",
    "    return norm\n",
    "\n",
    "def f1_keywords(preds: List[List[str]], refs: List[List[str]]) -> Dict[str, float]:\n",
    "    tp = fp = fn = 0\n",
    "    for p, r in zip(preds, refs):\n",
    "        ps = set(x.casefold() for x in p)\n",
    "        rs = set(x.casefold() for x in r)\n",
    "        tp += len(ps & rs); fp += len(ps - rs); fn += len(rs - ps)\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "def _clean_single(t: str) -> str:\n",
    "    if not isinstance(t, str):\n",
    "        return t\n",
    "    # Remove ASCII control chars except '\\n' (0x0A)\n",
    "    t = re.sub(r'[\\x00-\\x09\\x0B-\\x0C\\x0E-\\x1F\\x7F]', '', t)\n",
    "    # Remove CR and TAB explicitly; keep '\\n'\n",
    "    t = t.replace('\\r', '').replace('\\t', ' ')\n",
    "    # Collapse spaces (but not '\\n')\n",
    "    t = re.sub(r'[ ]{2,}', ' ', t)\n",
    "    # Trim per line\n",
    "    t = '\\n'.join(line.strip() for line in t.split('\\n'))\n",
    "    return t.strip()\n",
    "\n",
    "def clean_key_phrase(x: Union[str, List[str]]) -> Union[str, List[str]]:\n",
    "    \"\"\"Clean a string or list[str] of keywords; preserves '\\n' in strings.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return [_clean_single(t) for t in x if isinstance(t, str) and t.strip()]\n",
    "    return _clean_single(x)\n",
    "\n",
    "def _dedup_preserve_order(items: List[str]) -> List[str]:\n",
    "    \"\"\"Case-insensitive, order-preserving dedup (keeps first-seen casing).\"\"\"\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for it in items:\n",
    "        if not isinstance(it, str):\n",
    "            continue\n",
    "        k = it.strip()\n",
    "        if not k:\n",
    "            continue\n",
    "        kn = k.casefold()  # Unicode-safe\n",
    "        if kn not in seen:\n",
    "            seen.add(kn)\n",
    "            out.append(k)\n",
    "    return out\n",
    "\n",
    "def build_hf_dataset_from_pandas(df: pd.DataFrame, seed: int = 42) -> DatasetDict:\n",
    "\n",
    "    df = df.copy()\n",
    "    # Clean text (optional; comment out next line if you don't want it)\n",
    "    df[\"text\"] = df[\"text\"].astype(str).map(_clean_single)\n",
    "\n",
    "    def to_keywords(v) -> List[str]:\n",
    "        # split to list if string; already list => keep\n",
    "        kws = v.split(\"\\n\") if isinstance(v, str) else (v if isinstance(v, list) else [])\n",
    "        kws = clean_key_phrase(kws) or []\n",
    "        kws = [k for k in kws if k]  # drop empties\n",
    "        kws = _dedup_preserve_order(kws)\n",
    "        return kws\n",
    "\n",
    "    df[\"keywords\"] = df[\"key\"].apply(to_keywords)\n",
    "    df = df[df[\"keywords\"].map(len) > 0].reset_index(drop=True)\n",
    "\n",
    "    features = Features({\n",
    "        \"text\": Value(\"string\"),\n",
    "        \"keywords\": Sequence(Value(\"string\")),\n",
    "    })\n",
    "\n",
    "    ds_all = Dataset.from_pandas(\n",
    "        df[[\"text\", \"keywords\"]], preserve_index=False, features=features\n",
    "    ).shuffle(seed=seed)\n",
    "\n",
    "    split = ds_all.train_test_split(test_size=0.2, seed=seed)\n",
    "    val_test = split[\"test\"].train_test_split(test_size=0.5, seed=seed)\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": split[\"train\"],\n",
    "        \"validation\": val_test[\"train\"],\n",
    "        \"test\": val_test[\"test\"],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a817f753-ccf1-467e-80eb-ee735878fd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 2799\n",
      "Number of validation samples: 350\n",
      "Number of test samples: 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2799/2799 [00:00<00:00, 11156.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 350/350 [00:00<00:00, 4525.34 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 350/350 [00:00<00:00, 4655.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "pd_df = pd.read_parquet(configobj.data_dir / \"kw_raw.parquet\")\n",
    "kw_ds = build_hf_dataset_from_pandas(pd_df)\n",
    "print(f\"Number of training samples: {len(kw_ds['train'])}\")\n",
    "print(f\"Number of validation samples: {len(kw_ds['validation'])}\")\n",
    "print(f\"Number of test samples: {len(kw_ds['test'])}\")\n",
    "kw_ds.save_to_disk(configobj.data_dir / 'keyword_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfe98f9a-1ae8-450f-85fb-010a194ce019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'keywords'],\n",
       "        num_rows: 2799\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'keywords'],\n",
       "        num_rows: 350\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'keywords'],\n",
       "        num_rows: 350\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a2dde2-ca2f-4b3f-abf0-cff4bfd466a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordDataModule:\n",
    "    def __init__(self, data_cfg: DataConfig, prompt_cfg: PromptConfig):\n",
    "        self.cfg = data_cfg\n",
    "        self.prompt = prompt_cfg\n",
    "\n",
    "    def _to_keyword_list(self, v: Any) -> List[str]:\n",
    "        if v is None:\n",
    "            return []\n",
    "        if isinstance(v, list):\n",
    "            items = [str(x).strip() for x in v if isinstance(x, (str, int, float))]\n",
    "        else:\n",
    "            s = str(v)\n",
    "            parts = []\n",
    "            for chunk in s.split(\"\\n\"):\n",
    "                for p in chunk.split(\";\"):\n",
    "                    parts.extend(p.split(\",\"))\n",
    "            items = [p.strip() for p in parts]\n",
    "        seen, out = set(), []\n",
    "        for k in items:\n",
    "            if not k:\n",
    "                continue\n",
    "            kn = k.casefold()\n",
    "            if kn not in seen:\n",
    "                seen.add(kn)\n",
    "                out.append(k)\n",
    "        return out\n",
    "\n",
    "    def _format_example(self, ex: Dict[str, Any]) -> Dict[str, str]:\n",
    "        doc = (ex.get(self.cfg.text_field, \"\") or \"\").strip()\n",
    "        kw_list = self._to_keyword_list(ex.get(self.cfg.keywords_field, []))\n",
    "        labels = self.prompt.sep.join(kw_list)\n",
    "        prompt = (\n",
    "            f\"{self.prompt.system_preamble}\\n\\n\"\n",
    "            f\"DOCUMENT:\\n{doc}\\n\\n\"\n",
    "            f\"{self.prompt.response_tag}\"\n",
    "        )\n",
    "        return {\"text\": prompt, \"labels\": labels}\n",
    "\n",
    "    def load(self, dsd: Optional[DatasetDict] = None) -> DatasetDict:\n",
    "        if dsd is None:\n",
    "            if self.cfg.hf_path_or_none is None:\n",
    "                raise ValueError(\"Provide a DatasetDict or set DataConfig.hf_path_or_none.\")\n",
    "            try:\n",
    "                dsd = DatasetDict.load_from_disk(self.cfg.hf_path_or_none)\n",
    "            except Exception:\n",
    "                dsd = load_dataset(self.cfg.hf_path_or_none)\n",
    "\n",
    "        mapped = DatasetDict()\n",
    "        for split in dsd.keys():\n",
    "            ds = dsd[split]\n",
    "            if split == \"train\" and self.cfg.max_train_samples:\n",
    "                ds = ds.select(range(min(self.cfg.max_train_samples, len(ds))))\n",
    "            if split in (\"validation\", \"test\") and self.cfg.max_eval_samples:\n",
    "                ds = ds.select(range(min(self.cfg.max_eval_samples, len(ds))))\n",
    "            ds = ds.map(\n",
    "                self._format_example,\n",
    "                remove_columns=[c for c in ds.column_names if c not in (\"text\", \"labels\")],\n",
    "            )\n",
    "            ds = ds.filter(lambda ex: bool(ex[\"labels\"] and ex[\"labels\"].strip()))\n",
    "            mapped[split] = ds\n",
    "\n",
    "        for s in mapped.keys():\n",
    "            cols = set(mapped[s].column_names)\n",
    "            if not {\"text\", \"labels\"}.issubset(cols):\n",
    "                raise RuntimeError(f\"Split '{s}' must contain 'text' and 'labels', got: {cols}\")\n",
    "        return mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bac1416-a1b7-444c-8b8a-8cd96bd374be",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class KeywordCompletionCollator:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    response_template: str = \"KEYWORDS:\"\n",
    "    max_length: int = 2048\n",
    "    pad_to_multiple_of: Optional[int] = 8\n",
    "    add_eos: bool = True\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def _pad_right(\n",
    "        self,\n",
    "        seqs: List[List[int]],\n",
    "        pad_id: int,\n",
    "        max_len: int,\n",
    "        pad_multiple: Optional[int] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # truncate\n",
    "        seqs = [s[:max_len] for s in seqs]\n",
    "        tgt_len = max(len(s) for s in seqs) if seqs else 0\n",
    "        if pad_multiple and tgt_len % pad_multiple:\n",
    "            tgt_len = ((tgt_len + pad_multiple - 1) // pad_multiple) * pad_multiple\n",
    "        out = torch.full((len(seqs), tgt_len), pad_id, dtype=torch.long)\n",
    "        for i, s in enumerate(seqs):\n",
    "            out[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
    "        return out\n",
    "\n",
    "    def _get_text_labels(self, ex: Dict[str, Any]) -> Tuple[str, str]:\n",
    "        if \"text\" in ex and \"labels\" in ex:\n",
    "            return ex[\"text\"], ex[\"labels\"]\n",
    "        if \"prompt\" in ex and \"response\" in ex:\n",
    "            return ex[\"prompt\"], ex[\"response\"]\n",
    "        raise KeyError(f\"Example must contain ('text','labels') or ('prompt','response'), got: {list(ex.keys())}\")\n",
    "\n",
    "    # ---------- main ----------\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # Case A: pre-tokenized items: {'input_ids': ..., 'labels': ...} (attention_mask optional)\n",
    "        if \"input_ids\" in batch[0]:\n",
    "            pad_id = self.tokenizer.pad_token_id\n",
    "            if pad_id is None:\n",
    "                # ensure pad token exists\n",
    "                pad_id = self.tokenizer.eos_token_id or 0\n",
    "\n",
    "            # Collect fields\n",
    "            input_ids = [ex[\"input_ids\"] for ex in batch]\n",
    "            labels_in = [ex.get(\"labels\", None) for ex in batch]\n",
    "            attn = [ex.get(\"attention_mask\", None) for ex in batch]\n",
    "\n",
    "            # Truncate/pad\n",
    "            input_ids_t = self._pad_right(input_ids, pad_id=pad_id, max_len=self.max_length,\n",
    "                                          pad_multiple=self.pad_to_multiple_of)\n",
    "\n",
    "            # attention_mask: 1 where not pad\n",
    "            if any(a is not None for a in attn):\n",
    "                # If some provided, recompute from padded input_ids to be safe\n",
    "                attention_mask_t = (input_ids_t != pad_id).long()\n",
    "            else:\n",
    "                attention_mask_t = (input_ids_t != pad_id).long()\n",
    "\n",
    "            # labels: if list[int] provided, pad with -100; else create ignore-only labels\n",
    "            labels_list: List[List[int]] = []\n",
    "            for ids, lab in zip(input_ids, labels_in):\n",
    "                if isinstance(lab, list) and all(isinstance(x, int) for x in lab):\n",
    "                    labels_list.append(lab)\n",
    "                else:\n",
    "                    # default: ignore loss everywhere (trainer may not expect this, but safe)\n",
    "                    labels_list.append([-100] * len(ids))\n",
    "\n",
    "            labels_t = self._pad_right(labels_list, pad_id=-100, max_len=self.max_length,\n",
    "                                       pad_multiple=self.pad_to_multiple_of)\n",
    "\n",
    "            # If some labels seqs are shorter than inputs, ensure padding positions are -100\n",
    "            if labels_t.shape[1] < input_ids_t.shape[1]:\n",
    "                # expand labels to match inputs\n",
    "                expanded = torch.full_like(input_ids_t, -100)\n",
    "                expanded[:, :labels_t.shape[1]] = labels_t\n",
    "                labels_t = expanded\n",
    "            elif labels_t.shape[1] > input_ids_t.shape[1]:\n",
    "                labels_t = labels_t[:, :input_ids_t.shape[1]]\n",
    "\n",
    "            return {\n",
    "                \"input_ids\": input_ids_t,\n",
    "                \"attention_mask\": attention_mask_t,\n",
    "                \"labels\": labels_t,\n",
    "            }\n",
    "\n",
    "        # Case B: string format -> build prompt + labels and tokenize here\n",
    "        prompts, targets = [], []\n",
    "        for ex in batch:\n",
    "            p, t = self._get_text_labels(ex)\n",
    "            prompts.append(p)\n",
    "            targets.append(\"\" if t is None else t)\n",
    "\n",
    "        eos = self.tokenizer.eos_token if (self.add_eos and self.tokenizer.eos_token) else \"\"\n",
    "        full_texts = [\n",
    "            p + (\" \" if (t and not p.endswith(\" \")) else \"\") + t + eos\n",
    "            for p, t in zip(prompts, targets)\n",
    "        ]\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            full_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        attention_mask = enc[\"attention_mask\"]\n",
    "\n",
    "        # Find boundary after \"prompt + response_template\"\n",
    "        tagged_prompts = [\n",
    "            p if p.strip().endswith(self.response_template)\n",
    "            else (p.rstrip() + \" \" + self.response_template)\n",
    "            for p in prompts\n",
    "        ]\n",
    "        tag_enc = self.tokenizer(\n",
    "            tagged_prompts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=input_ids.size(1),\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        tag_lens = tag_enc[\"attention_mask\"].sum(dim=1)\n",
    "        seq_lens = attention_mask.sum(dim=1)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[:] = -100\n",
    "        for i in range(input_ids.size(0)):\n",
    "            start = int(tag_lens[i].item())\n",
    "            end = int(seq_lens[i].item())\n",
    "            if start < end:\n",
    "                labels[i, start:end] = input_ids[i, start:end]\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3fb5fa7-ee14-4590-9e9e-c78f7b992ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRALoader:\n",
    "    def __init__(self, cfg: TrainConfig):\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "\n",
    "    def _assert_no_bnb(self):\n",
    "        # If bitsandbytes is importable, PEFT will try to use its path and load Triton/CUDA headers.\n",
    "        try:\n",
    "            import bitsandbytes  # noqa: F401\n",
    "            raise RuntimeError(\n",
    "                \"bitsandbytes is installed but 4-bit is disabled. \"\n",
    "                \"Uninstall bitsandbytes (pip uninstall -y bitsandbytes) to avoid cuda.h build.\"\n",
    "            )\n",
    "        except Exception:\n",
    "            # OK: not installed (ImportError) or we raised above\n",
    "            pass\n",
    "\n",
    "    def _guess_target_modules(self, model: torch.nn.Module) -> List[str]:\n",
    "        candidates = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "        present, seen, uniq = [], set(), []\n",
    "        for n, _ in model.named_modules():\n",
    "            for c in candidates:\n",
    "                if n.endswith(c):\n",
    "                    present.append(c)\n",
    "        for x in present:\n",
    "            if x not in seen:\n",
    "                seen.add(x); uniq.append(x)\n",
    "        return uniq or [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "    def load(self):\n",
    "        print(f\"Loading tokenizer: {self.cfg.model_id}\")\n",
    "        tok = AutoTokenizer.from_pretrained(self.cfg.model_id, use_fast=True)\n",
    "        tok.padding_side = \"right\"\n",
    "        if tok.pad_token is None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "\n",
    "        print(f\"Loading model: {self.cfg.model_id}\")\n",
    "        # Force-disable 4-bit completely\n",
    "        if self.cfg.load_in_4bit:\n",
    "            print(\"[INFO] Overriding: load_in_4bit requested but disabled due to CUDA header issues.\")\n",
    "        self.cfg.load_in_4bit = False\n",
    "        self._assert_no_bnb()  # ensure PEFT won’t import its bnb path\n",
    "\n",
    "        dtype = torch.bfloat16 if self.cfg.bf16 else torch.float16\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.cfg.model_id,\n",
    "            dtype=dtype,\n",
    "            device_map=\"auto\",   # do NOT call .to('cuda') after this\n",
    "        )\n",
    "\n",
    "        target_modules = self.cfg.target_modules or self._guess_target_modules(model)\n",
    "        print(\"Using LoRA target_modules:\", target_modules)\n",
    "        lora_cfg = LoraConfig(\n",
    "            r=self.cfg.lora_r,\n",
    "            lora_alpha=self.cfg.lora_alpha,\n",
    "            lora_dropout=self.cfg.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            target_modules=target_modules,\n",
    "        )\n",
    "        model = get_peft_model(model, lora_cfg)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "        self.tokenizer, self.model = tok, model\n",
    "        return tok, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47645e54-ce21-4f0c-af49-d16d5d428bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KwGenTrainer:\n",
    "    def __init__(self, train_cfg: TrainConfig, prompt_cfg: PromptConfig):\n",
    "        self.cfg = train_cfg\n",
    "        self.prompt = prompt_cfg\n",
    "        self.trainer = None\n",
    "\n",
    "    def build(self, tok, model, dsd_mapped: DatasetDict):\n",
    "        collator = KeywordCompletionCollator(\n",
    "            tokenizer=tok,\n",
    "            response_template=self.prompt.response_tag,\n",
    "            max_length=self.cfg.max_seq_len,   # <- collator enforces seq length\n",
    "            pad_to_multiple_of=8,\n",
    "        )\n",
    "    \n",
    "        sft_args = SFTConfig(\n",
    "            output_dir=self.cfg.output_dir,\n",
    "            packing=False,\n",
    "            per_device_train_batch_size=self.cfg.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=self.cfg.gradient_accumulation_steps,\n",
    "            num_train_epochs=self.cfg.num_train_epochs,\n",
    "            learning_rate=self.cfg.learning_rate,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            logging_steps=self.cfg.logging_steps,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=self.cfg.eval_steps,\n",
    "            save_steps=self.cfg.save_steps,\n",
    "            warmup_ratio=self.cfg.warmup_ratio,\n",
    "            weight_decay=self.cfg.weight_decay,\n",
    "            bf16=self.cfg.bf16,\n",
    "            fp16=self.cfg.fp16,\n",
    "            optim=getattr(self.cfg, \"optim\", \"adamw_torch\"),\n",
    "            report_to=self.cfg.report_to,\n",
    "            seed=self.cfg.seed,\n",
    "            gradient_checkpointing=self.cfg.gradient_checkpointing,\n",
    "            save_total_limit=self.cfg.save_total_limit,\n",
    "        )\n",
    "    \n",
    "        # IMPORTANT: do NOT pass tokenizer=, dataset_text_field=, or max_seq_length=\n",
    "        self.trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            args=sft_args,\n",
    "            train_dataset=dsd_mapped.get(\"train\"),\n",
    "            eval_dataset=dsd_mapped.get(\"validation\"),\n",
    "            data_collator=collator,\n",
    "        )\n",
    "        return self.trainer\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_keywords(self, tok, model, eval_ds: Dataset, sample_size: int = 512, gen_kwargs=None) -> Dict[str, float]:\n",
    "        if gen_kwargs is None:\n",
    "            gen_kwargs = dict(max_new_tokens=96, temperature=0.2, do_sample=False, top_p=1.0, repetition_penalty=1.05)\n",
    "\n",
    "        n = min(sample_size, len(eval_ds))\n",
    "        prompts = [eval_ds[i][\"text\"] for i in range(n)]\n",
    "        refs = [eval_ds[i][\"labels\"] for i in range(n)]\n",
    "\n",
    "        preds_kw, refs_kw = [], []\n",
    "        model.eval()\n",
    "        device = model.device if hasattr(model, \"device\") else None\n",
    "\n",
    "        for p, r in zip(prompts, refs):\n",
    "            inputs = tok(p, return_tensors=\"pt\")\n",
    "            if device is not None:\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            out = model.generate(**inputs, **gen_kwargs)\n",
    "            gen_txt = tok.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "            preds_kw.append(normalize_kw_string(gen_txt))\n",
    "            refs_kw.append(normalize_kw_string(r))\n",
    "\n",
    "        return f1_keywords(preds_kw, refs_kw)\n",
    "\n",
    "    def train_and_eval(self, tok, model, dsd_mapped: DatasetDict) -> Dict[str, float]:\n",
    "        self.build(tok, model, dsd_mapped)\n",
    "        # Enable fast downloads if user has hf-transfer installed\n",
    "        os.environ.setdefault(\"HF_HUB_ENABLE_HF_TRANSFER\", \"1\")\n",
    "        self.trainer.train()\n",
    "\n",
    "        if \"validation\" in dsd_mapped and len(dsd_mapped[\"validation\"]) > 0:\n",
    "            metrics = self.evaluate_keywords(tok, self.trainer.model, dsd_mapped[\"validation\"])\n",
    "        else:\n",
    "            metrics = {\"precision\": float(\"nan\"), \"recall\": float(\"nan\"), \"f1\": float(\"nan\")}\n",
    "\n",
    "        adapter_dir = os.path.join(self.cfg.output_dir, \"adapter\")\n",
    "        self.trainer.model.save_pretrained(adapter_dir)\n",
    "        tok.save_pretrained(adapter_dir)\n",
    "        with open(os.path.join(self.cfg.output_dir, \"metrics.json\"), \"w\") as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        return metrics\n",
    "\n",
    "class KeywordGenPipeline:\n",
    "    def __init__(self, data_cfg: DataConfig, prompt_cfg: PromptConfig, train_cfg: TrainConfig):\n",
    "        self.data_cfg = data_cfg\n",
    "        self.prompt_cfg = prompt_cfg\n",
    "        self.train_cfg = train_cfg\n",
    "        self.data_module = KeywordDataModule(data_cfg, prompt_cfg)\n",
    "        self.loader = QLoRALoader(train_cfg)\n",
    "        self.runner = KwGenTrainer(train_cfg, prompt_cfg)\n",
    "\n",
    "    def run(self, dsd: Optional[DatasetDict] = None) -> Dict[str, float]:\n",
    "        dsd_mapped = self.data_module.load(dsd)\n",
    "        tok, model = self.loader.load()\n",
    "        metrics = self.runner.train_and_eval(tok, model, dsd_mapped)\n",
    "        print(\"Final metrics:\", metrics)\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d9287e2-fc80-4817-b61e-5ecbb6ff91e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cfg = DataConfig(\n",
    "    hf_path_or_none=configobj.data_dir / 'keyword_dataset',\n",
    "    text_field=\"text\",\n",
    "    keywords_field=\"keywords\",\n",
    "    max_train_samples=None,\n",
    "    max_eval_samples=800,\n",
    ")\n",
    "prompt_cfg = PromptConfig(\n",
    "    system_preamble=\"You are an expert keyword generator. Extract concise, relevant keywords for the document below.\",\n",
    "    response_tag=\"KEYWORDS:\",\n",
    "    sep=\"; \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd314b5f-d0b5-43ef-8c7f-15b6b565fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cfg = TrainConfig(\n",
    "    model_id=\"Qwen/Qwen2.5-3B-Instruct\",   # swap here to try other models (Phi-3.5, Llama-3.2-3B, Gemma-2-2B, etc.)\n",
    "    output_dir=\"runs/qwen25_3b_kwgen\",\n",
    "    load_in_4bit=False,\n",
    "    lora_r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=None,                   # auto-detects q_proj/k_proj/... if None\n",
    "    max_seq_len=2048,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=50,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.0,\n",
    "    seed=42,\n",
    "    bf16=False, fp16=True,\n",
    "    report_to=None,\n",
    "    gradient_checkpointing=True,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f10f7d4a-edc9-4ffb-b232-48bca4d54e54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: Qwen/Qwen2.5-3B-Instruct\n",
      "Loading model: Qwen/Qwen2.5-3B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LoRA target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "trainable params: 29,933,568 || all params: 3,115,872,256 || trainable%: 0.9607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/700 40:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.960327</td>\n",
       "      <td>2126990.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m pipe = KeywordGenPipeline(data_cfg, prompt_cfg, train_cfg)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mKeywordGenPipeline.run\u001b[39m\u001b[34m(self, dsd)\u001b[39m\n\u001b[32m    100\u001b[39m dsd_mapped = \u001b[38;5;28mself\u001b[39m.data_module.load(dsd)\n\u001b[32m    101\u001b[39m tok, model = \u001b[38;5;28mself\u001b[39m.loader.load()\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_and_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdsd_mapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinal metrics:\u001b[39m\u001b[33m\"\u001b[39m, metrics)\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mKwGenTrainer.train_and_eval\u001b[39m\u001b[34m(self, tok, model, dsd_mapped)\u001b[39m\n\u001b[32m     85\u001b[39m tok.save_pretrained(adapter_dir)\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os.path.join(\u001b[38;5;28mself\u001b[39m.cfg.output_dir, \u001b[33m\"\u001b[39m\u001b[33mmetrics.json\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[43mjson\u001b[49m.dump(metrics, f, indent=\u001b[32m2\u001b[39m)\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "\u001b[31mNameError\u001b[39m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "pipe = KeywordGenPipeline(data_cfg, prompt_cfg, train_cfg)\n",
    "pipe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a85c547e-6b29-4c32-81a0-0116552c7ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = KeywordDataModule(data_cfg, prompt_cfg)\n",
    "dsd_mapped = data_module.load()\n",
    "train_ds = dsd_mapped[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5fcf9532-dfa3-4869-925a-7e624ccb984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Tokenizer used by the collator\n",
    "tok = AutoTokenizer.from_pretrained(train_cfg.model_id, use_fast=True)\n",
    "tok.padding_side = \"right\"\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d85b4a7-5f3e-4640-9dd8-defd21c1064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Collator (your class)\n",
    "collator = KeywordCompletionCollator(\n",
    "    tokenizer=tok,\n",
    "    response_template=prompt_cfg.response_tag,  # \"KEYWORDS:\"\n",
    "    max_length=train_cfg.max_seq_len,\n",
    "    pad_to_multiple_of=8,\n",
    "    add_eos=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0a75702-27ac-4848-a2b8-b253a905193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Grab one batch\n",
    "loader = DataLoader(train_ds, batch_size=2, shuffle=False, collate_fn=collator)\n",
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05273de4-0ee2-4e85-960b-54d11a9ef9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Inspect decoded inputs and supervised tokens\n",
    "def show_example(i: int):\n",
    "    inp_ids = batch[\"input_ids\"][i]\n",
    "    lbl_ids = batch[\"labels\"][i]\n",
    "    attn    = batch[\"attention_mask\"][i]\n",
    "\n",
    "    full_text = tok.decode(inp_ids, skip_special_tokens=False)\n",
    "    target_text = tok.decode(lbl_ids[lbl_ids != -100], skip_special_tokens=False)\n",
    "\n",
    "    print(f\"\\n===== Example {i} =====\")\n",
    "    print(\"Ends with response tag? ->\", full_text.rstrip().endswith(prompt_cfg.response_tag))\n",
    "    print(\"\\n[Prompt+Target decoded]\\n\", full_text)\n",
    "    print(\"\\n[Supervised target (labels!=-100) decoded]\\n\", target_text)\n",
    "    print(\"\\n[counts]\",\n",
    "          \" total_tokens=\", int(attn.sum().item()),\n",
    "          \" supervised_tokens=\", int((lbl_ids != -100).sum().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29bdc5e7-5669-406a-a69e-5ebf6fc351ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Example 0 =====\n",
      "Ends with response tag? -> False\n",
      "\n",
      "[Prompt+Target decoded]\n",
      " You are an expert keyword generator. Extract concise, relevant keywords for the document below.\n",
      "\n",
      "DOCUMENT:\n",
      "Plasticity of the Human Auditory Cortex Induced by Discrimination Learning of Non-Native, Mora-Timed Contrasts of the Japanese Language\n",
      "In this magnetoencephalographic (MEG) study, we examined with high temporal resolution the traces of learning in the speech-dominant left-hemispheric auditory cortex as a function of newly trained mora-timing. In Japanese, the \"mora\" is a temporal unit that divides words into almost isochronous segments (e.g., na-ka-mu-ra and to-o-kyo-o each comprises four mora). Changes in the brain responses of a group of German and Japanese subjects to differences in the mora structure of Japanese words were compared. German subjects performed a discrimination training in 10 sessions of 1.5 h each day. They learned to discriminate Japanese pairs of words (in a consonant, anni --ani; and a vowel, kiyo --kyo, condition), where the second word was shortened by one mora in eight steps of 15 msec each. A significant increase in learning performance, as reflected by behavioral measures, was observed, accompanied by a significant increase of the amplitude of the Mismatch Negativity Field (MMF). The German subjects' hit rate for detecting durational deviants increased by up to 35%. Reaction times and MMF latencies decreased significantly across training sessions. Japanese subjects showed a more sensitive MMF to smaller differences. Thus, even in young adults, perceptual learning of non-native mora-timing occurs rapidly and deeply. The enhanced behavioral and neurophysiological sensitivity found after training indicates a strong relationship between learning and (plastic) changes in the cortical substrate.\n",
      "\n",
      "Our brain is a highly adaptive learning device, reorganizing itself in accordance with environmental constraints. Nevertheless, adult people experience difficulties when learning a language that has a completely new and unfamiliar phonemic structure. Language perception is altered by the linguistic experience we make throughout our life. We perceive and produce speech through the filter of our native language. Whereas neonates are able to discriminate most phonetic categories in the first months of life , they develop prototypical phoneme representations of their native language in the first year of life . Exposure to a specific language in the first half year of life directs infants' phonetic perception toward this language and forms prototypes for each category. A prototype functions like a perceptual magnet for all members of that category and attracts all variations of a phoneme into \"native\" perception . Once established, these phoneme categories form a stable basis for consecutive speech perception and speech production . As a consequence, adults have difficulties in distinguishing non-native phonemic contrasts. For instance, Finnish and Estonian adults perceive their native phoneme prototypes more sensitively than non-native phonemes . Japanese listeners have difficulties in distinguishing /l/ and /r/ and perceive both phonetic categories as one . Nevertheless, after extended training, adult Japanese listeners improved their perceptual identification as well as their production performance of English /r/ and /l/ . In another experiment, the behavioral training of just-perceptible differences in speech stimuli resulted in a significant change in the duration and magnitude of the cortical potentials . It is generally assumed that representational maps in the brain are subject to plastic changes subsequent to altering sensory input. Intensive frequency-discrimination training enhances the area of representation of the trained frequency range in the primary auditory cortex of owl monkeys . In humans, changes in the perceptual acuity of frequency discrimination correlate with enhancements in the neuromagnetic responses to these frequencies . Moreover, intensive phonetic experience of speech sound discrimination is apt to alter the neural activity that underlies coding of these events in infants , as well as in adults . showed that learning a new language is accompanied by the development of new cortical representations for unknown phonemes of the new language. Hungarians, who learned to speak Finnish fluently in adulthood, developed cortical memory representations for Finnish phoneme categories that do not exist in their native language. Phonemes are the smallest units of speech that affect meaning. A small difference in the phonemic structure of a word entirely changes its meaning. Each language has its own \"music,\" that is, languages differ in their rhythm and stress patterns. Language has a certain rhythm or timing because phonological units (i.e., syllables or segments like moras) are organized into rhythmic sequences. Whereas English is generally considered as a stress-timed language and French as a syllable-timed language, Japanese and even Finnish are often cited as mora-timed languages . In Japanese, the \"mora\" is a temporal unit that divides words into almost isochronous segments (e.g., na-ka-mu-ra and to-o-kyo-o each comprises 4 mora). For Japanese and Finnish, the relative duration of a vowel or consonant is a decisive feature for differentiating among meaningful words. Nevertheless, in stress-timed languages, for instance in English, the difference between heed and hid, bead and bid, or wooed and wood is sometimes only encoded by the length of the vowel . In the German language, vowel duration can also be decisive for the meaning of a word (as in biete and bitte, Miete and Mitte), but is additionally accompanied by a different accentuation or consonant duration that facilitates the perception of the difference. However, in languages such as Finnish, Estonian, Hungarian, and Japanese, every short vowel has a corresponding long counterpart. Similarly, the duration of some consonants has both a short and long variant. These durational contrasts are used to encode different meanings. For instance, in Finnish, all variants of these words, tule --tuule --tuulle --tuulee --tulee --tullee --tulle --tuullee, are to be distinguished accurately by their duration quantity because they encode different meanings. Mora-timing might also be relevant in Finnish . Enhanced processing of speech sounds, especially for duration differences as compared with equivalent changes in tones, was found for Finnish speakers . The Hungarian language also has many long/short differences to be distinguished (e.g., fulel --fullel, sor --so:r, megy --meggy). Recently, the moraic/nonmoraic distinction was postulated even in natural spoken Danish . Japanese is a mora-timed language par excellence ; its temporal units consist mostly of consonant --vowel (CV or CVV) syllables, single vowels, or the nasal /n/. Each unit takes about the same length of time . Long syllables (ko-o) may consist of two moras, but short syllables (ko) only consist of one. These units are expressed in the two phonetic alphabets hiragana and katakana with maximally 111 possible Japanese CV (ka, ba, ra) or CyV monomoraic (kyo, byo, ryo) syllables, five vowels, and the consonant /n/, and form the entire set of elementary phonetic units of Japanese. Thus, the rhythm of Japanese is controlled by a regular temporal sequence, mostly isochronous. The Japanese language has a restricted number of moras, which results in a large number of homonyms. These homonyms are often differentiated only by the duration of a mora when the context is not available. For instance, the sound sequence /hoshu/ exists in four variants: with a long /o/, with a long /u/, with both a long /o/ and a long /u/, or with both a short /o/ and a short /u/. The words hoshu, hooshu, hoshuu, and hooshuu have completely different meanings: hoshu means \"a catcher\" or \"conservative\"; hooshu means \"a gunner\" or \"artilleryman\"; hoshuu means \"mend, repair\"; and hooshuu means \"remuneration, reward.\" The same is true for a large number of other Japanese words. Some of these distinctions as in ojiisan (grandfather) and ojisan (uncle) or obaasan (grandmother) and obasan (aunt) can lead to embarrassing misunderstandings, if the duration of the vowel is not considered precisely. studied the role of mora-timing in the recognition of spoken Japanese words. They showed that Japanese listeners are sensitive to the moraic structure of speech and that it is easier for them to respond to moras than to phonemes. even concluded that Japanese listeners parse the speech signal into units larger than phonemes, namely, moras, because they mainly use the prevailing moraic consonant --vowel (CV) unit in current speech. The mora-unspecific VC cues are only used in natural VCV words where the consonant is the onset of a CV mora. This has led to several questions: (1) What are the effects of long-term experience with one's native language, when this language affords accurate temporal discrimination? (2) What kind of changes can be produced by intensive auditory discrimination training of non-native durational differences, and what are the related neurophysiological correlates? (3) What is the difference between native and non-native perception and discrimination learning of durational differences within one mora? To answer these questions, we compared the perception and processing of Japanese words in German and Japanese subjects. The duration quantity in bi- and trimoraic words was manipulated in 8 steps over the length of one mora. German subjects (test group) were trained to discriminate these differences. The control group was composed of Japanese native speakers. Accurate perception of non-native phonemic categories can be quantified by means of an automatic, preattentive \"change detector\" component of the Auditory Evoked Response (AER), called the Mismatch Negativity (MMN) and its magnetic counterpart, the Mismatch Field\n",
      "\n",
      "[Supervised target (labels!=-100) decoded]\n",
      " \n",
      "\n",
      "[counts]  total_tokens= 2048  supervised_tokens= 0\n",
      "\n",
      "===== Example 1 =====\n",
      "Ends with response tag? -> False\n",
      "\n",
      "[Prompt+Target decoded]\n",
      " You are an expert keyword generator. Extract concise, relevant keywords for the document below.\n",
      "\n",
      "DOCUMENT:\n",
      "--T\n",
      "On-the-Fly Model Checking Under Fairness that Exploits Symmetry.\n",
      "--A\n",
      "An on-the-fly algorithm for model checking under fairness is\n",
      "presented. The algorithm utilizes symmetry in the program to\n",
      "reduce the state space, and employs novel techniques that make the\n",
      "on-the-fly model checking feasible. The algorithm uses state symmetry and\n",
      "eliminates parallel edges in the reachability graph. Experimental results\n",
      "demonstrating dramatic reductions in both the running time and memory\n",
      "usage are presented.\n",
      "--B\n",
      "Introduction\n",
      "The state explosion problem is one of the major bottlenecks in temporal logic model\n",
      "checking. Many techniques have been proposed in the literature [6, 5, 9, 8, 13, 11,\n",
      "12, 16, 17] for combating this problem. Among these, symmetry based techniques\n",
      "have been proposed in [5, 9, 13]. In these methods the state space of a program\n",
      "is collapsed by identifying states that are equivalent under symmetry and model\n",
      "checking is performed on the reduced graph. Although the initial methods of [5, 9]\n",
      "could only handle a limited set of liveness properties, a more generalized approach\n",
      "for checking liveness properties under various notions of fairness has been proposed\n",
      "in [10]. This method, however, does not facilitate early termination, it supplies an\n",
      "answer only after the construction of all the required data structures is complete.\n",
      "Many traditional model checking algorithms ([3, 11, 12, 17]) use on-the-fly techniques\n",
      "to avoid storing the complete state space in the main memory. However,\n",
      "none of these techniques employ symmetry. [13] uses on-the-fly techniques together\n",
      "with symmetry for model checking. There the focus is on reasoning about a simple\n",
      "but basic type of correctness, i.e., safety properties expressible in the temporal logic\n",
      "CTL by an assertion of the form AG:error.\n",
      "In this paper, we present an on-the-fly model checking algorithm that checks for\n",
      "correctness under weak fairness and that exploits symmetry. computation is said\n",
      "* A preliminary version of this paper appeared in the Proceedings of the 9th International\n",
      "Conference on Computer Aided Verification held in Haifa, Israel in June 1997. The work presented\n",
      "in this paper is partially supported by the NSF grants CCR-9623229 and CCR-9633536\n",
      "to be weakly fair if every process is either infinitely often disabled or is executed\n",
      "infinitely often). This work is an extension of the work presented in [10]. Here\n",
      "we develop additional theory leading to novel techniques that make the on-the-\n",
      "fly model-checking feasible. We not only exploit the symmetry between different\n",
      "states, but also take advantage of the symmetric structure of each individual state;\n",
      "this allows us to further reduce the size of the explored state space.\n",
      "The other major improvement is gained by breaking the sequential line of the\n",
      "algorithm. The original algorithm constructed three data structures (the reduced\n",
      "state space, the product graph and the threaded graph - details are given below)\n",
      "one after the other and performed a test on the last one. We eliminated the construction\n",
      "of the third data structure by maintaining some new dynamic information;\n",
      "the algorithm constructs parts of the first structure only when it is needed in the\n",
      "construction of the second; finally we store only the nodes in the second structure.\n",
      "This on-the-fly construction technique and up-to-date dynamic information maintenance\n",
      "facilitates early termination if the program does not satisfy the correctness\n",
      "specification and allows us to construct only the minimal necessary portion of the\n",
      "state space when the program satisfies the correctness specification. The on-the-fly\n",
      "model checking algorithm has been implemented and experimental results indicate\n",
      "substantial improvement in performance compared to the original method.\n",
      "The algorithm, given in [10], works as follows. It assumes that the system consists\n",
      "of a set I of processes that communicate through shared variables. Each variable is\n",
      "associated with a subset of I , called the index set, that denotes the set of processes\n",
      "that share the variable. Clearly, the index set of a local variable consists of a single\n",
      "process only. A state of the system is a mapping that associates appropriate values\n",
      "to the variables. A permutation - over the set I of processes, extends naturally\n",
      "to a permutation over the set of variables and to the states of the system. A\n",
      "permutation - is an automorphism of the system if the reachability graph of the\n",
      "system is invariant under - (more specifically, if s ! t is an edge in the reachability\n",
      "graph then -(s) ! -(t) is also an edge and vice versa). Two states are equivalent\n",
      "if there is an automorphism of the system that maps one to the other. Factoring\n",
      "with this equivalence relation compresses the reachable state space.\n",
      "The original method consists of three phases. First it constructs the reduced\n",
      "state space. Then it computes the product of the reduced state space and the finite\n",
      "state automaton that represents the set of incorrect computations. It explores\n",
      "the product graph checking for existence of \"fair\" and \"final\" strongly connected\n",
      "components; these components correspond to fair incorrect computations. Checking\n",
      "if a strongly connected component is fair boils down to checking if it is fair with\n",
      "respect to each individual process. This is done by taking the product of the\n",
      "component and the index set I . The result is called the threaded graph resolution\n",
      "of the component. A path in the threaded graph corresponds to a computation of\n",
      "the system with special attention to one designated process. Fairness of a strongly\n",
      "connected component in the product graph is checked by verifying that each of the\n",
      "strongly connected components of its threaded graph are fair with respect to the\n",
      "designated process of that component.\n",
      "Our on-the-fly algorithm has two layers: the reduced state space and the product\n",
      "graph construction. The successors of a node in the reduced state space are constructed\n",
      "only when the product graph construction requests it. The product graph\n",
      "construction is engined by a modified algorithm for computing strongly connected\n",
      "components (scc) using depth first search (see [2]). During the depth first search,\n",
      "with each vertex on the stack it maintains a partition vector of the process set I .\n",
      "The partition vector associated to a product state u captures information about\n",
      "the threaded graph of the strongly connected component of u in the already explored\n",
      "part of the product graph. Intuitively, if processes i and j are in the same\n",
      "partition class then it indicates that the nodes (u; i) and (u; are in the same scc\n",
      "of the threaded graph. This reveals that the infinite run of the system corresponding\n",
      "to the strongly connected component of u in the already explored part of the\n",
      "product graph is either fair with respect to both processes i and j or it is not fair\n",
      "with respect to any of the mentioned processes. The partition vectors are updated\n",
      "whenever a new node or an edge to an already constructed node is explored. The\n",
      "correctness of the above algorithm is based on new theory that we develop as part\n",
      "of this paper; this thoery connects the partition vectors of the algorithm with the\n",
      "strongly connected components of the threaded graph.\n",
      "A permutation - 2 AutM on processes is a state symmetry of a state s, if\n",
      "(state symmetry was originally introduced in [9]). Suppose that - maps process i\n",
      "to j. In that case, transitions ignited by process i are in one-to-one correspondence\n",
      "with those caused by process j. Hence, we can save space and computation time\n",
      "by considering only those that belong to process i. This is one of the forms state\n",
      "symmetry is exploited in our algorithm. Another way is the initialization of the\n",
      "partition vector with the state symmetry partition. If - maps process i to j then\n",
      "the threads corresponding to process i and j are certainly in the same situation:\n",
      "they are either both fair or none is fair.\n",
      "Our paper is organized as follows. Section 2 contains notation and preliminaries.\n",
      "In Sect. 3 we develop the necessary theory and present the on-the-fly algorithm.\n",
      "We describe various modifications of the algorithm that take state symmetry into\n",
      "consideration. Section 4 presents experimental results showing the effectiveness of\n",
      "our algorithm and dramatic improvements in time as well as memory usage. Section\n",
      "5 contains concluding remarks.\n",
      "2. Preliminaries\n",
      "2.1. Programs, Processes, Global State Graph\n",
      "Let I be a set of process indices. We consider a system\n",
      "running in parallel. Each process K i is a set of transitions. We assume that\n",
      "all variables of P are indexed by a subset of I indicating which processes share\n",
      "the variable. A system P , that meets the above description, is called an indexed\n",
      "transition system (or briefly program).\n",
      "A global state of an indexed transition system is an assignment of values to the\n",
      "variables. We assume that each variable can take only a finite number of values.\n",
      "This assumption ensures that the number of global states of the system is also\n",
      "finite. We define an indexed graph M on the set of global states that captures the\n",
      "behavior of the program. The indexed global state graph is\n",
      "is the set of global states, s 0 is the initial state, and R ' S \\Theta I \\Theta S is the transition\n",
      "relation, i.e., s i\n",
      "there is a transition in process i that is enabled in state\n",
      "s and its execution leads to state t.\n",
      "2.2. Strongly Connected Subgraphs and Weak Fairness\n",
      "Infinite paths in M starting from the initial state denote computations of P . An\n",
      "infinite path p in M is weakly fair if for each process i, either i is disabled infinitely\n",
      "often in p or it is executed infinitely often in p. Unless otherwise stated, we only\n",
      "consider weak fairness throughout the paper. The implementation, however\n",
      "\n",
      "[Supervised target (labels!=-100) decoded]\n",
      " \n",
      "\n",
      "[counts]  total_tokens= 2048  supervised_tokens= 0\n"
     ]
    }
   ],
   "source": [
    "show_example(0)\n",
    "if batch[\"input_ids\"].size(0) > 1:\n",
    "    show_example(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0cb04-abb3-4c81-94d5-1b80986634bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KeywordGen (Py3.12)",
   "language": "python",
   "name": "keywordgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
